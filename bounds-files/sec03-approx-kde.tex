\citet{ram2009} present a clever technique for bounding the
running time of approximate kernel density estimation based on the properties of
the kernel, when the kernel is shift-invariant and satisfies a few assumptions.
We will restate these assumptions and provide an adapted proof using Theorem
\ref{thm:ct-runtime}, which gives a tighter bound.

Approximate kernel density estimation is a common application of dual-tree
algorithms \citep{gray2003nonparametric, nbody}.  Given a query set $S_q$, a
reference set $S_r$ of size $N$, and a kernel function $\mathcal{K}(\cdot,
\cdot)$, the true kernel density estimate for a query point $p_q$ is given as

\begin{equation}
f^*(p_q) = \sum_{p_r \in S_r} \mathcal{K}(p_q, p_r).
\end{equation}

In the case of an infinite-tailed kernel $\mathcal{K}(\cdot, \cdot)$, the
exact computation cannot be accelerated; thus, attention has turned towards
tractable approximation schemes.  Two simple schemes for the approximation of
$f^*(p_q)$ are well-known: {\it absolute value approximation} and {\it relative
value approximation}.  Absolute value approximation requires that each density
estimate $f(p_q)$ is within $\epsilon$ of the true estimate $f^*(p_q)$:

\begin{equation}
\label{eqn:ava}
| f(p_q) - f^*(p_q) | < \epsilon \; \; \forall p_q \in S_q.
\end{equation}

Relative value approximation is a more flexible approximation scheme; given some
parameter $\epsilon$, the requirement is that each density estimate is within a
relative tolerance of $f^*(p_q):$

\begin{equation}
\label{eqn:rva}
\frac{| f(p_q) - f^*(p_q) |}{| f^*(p_q) |} < \epsilon \; \; \forall p_q \in S_q.
\end{equation}

Kernel density estimation is related to the well-studied problem of kernel
summation, which can also be solved with dual-tree algorithms
\citep{lee2006faster, lee2008fast}.  In both of those problems, regardless of
the approximation scheme, simple geometric observations can be made to
accelerate computation: when $\mathcal{K}(\cdot, \cdot)$ is shift-invariant,
faraway points have very small kernel evaluations.  Thus, trees can be built on
$S_q$ and $S_r$, and node combinations can be pruned when the nodes are far
apart while still obeying the error bounds.

In the following two subsections, we will separately consider both the absolute
value approximation scheme and the relative value approximation scheme, under
the assumption of a shift-invariant kernel $\mathcal{K}(p_q, p_r) =
\mathcal{K}(\| p_q - p_r \|)$ which is monotonically decreasing and
non-negative.  In addition, we assume that there exists some bandwidth $h$ such
that $\mathcal{K}(d)$ must be concave for $d \in [0, h]$ and convex for $d \in
[h, \infty)$.  This assumption implies that the magnitude of the derivative
$|\mathcal{K}'(d)|$ is maximized at $d = h$.  These are not restrictive
assumptions; most standard kernels fall into this class, including the Gaussian,
exponential, and Epanechnikov kernels.

\subsection{Absolute value approximation}

A tree-independent algorithm for solving approximate kernel density estimation
with absolute value approximation under the previous assumptions on the kernel
is given as a \texttt{BaseCase()} function in Algorithm \ref{alg:kde_base_case}
and a \texttt{Score()} function in Algorithm \ref{alg:kde_score}  \citep[a
correctness proof can be found in][]{curtin2013tree}.  The list $f_p$ holds
partial kernel density estimates for each query point, and the list $f_n$ holds
partial kernel density estimates for each query node.  At the beginning of the
dual-tree traversal, the lists $f_p$ and $f_n$, which are both of size $O(N)$,
are each initialized to 0.  As the traversal proceeds, node combinations are
pruned if the difference between the maximum kernel value
$\mathcal{K}(d_{\min}(\mathscr{N}_q, \mathscr{N}_r))$ and the minimum kernel
value $\mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r))$ is sufficiently
small (line \ref{alg:ava-kde-prune}).  If the node combination can be pruned,
then the partial node estimate is updated (line \ref{alg:ava-kde-update}).  When
node combinations cannot be pruned, \texttt{BaseCase()} may be called, which
simply updates the partial point estimate with the exact kernel evaluation (line
\ref{alg:kde-bc-update}).

After the dual-tree traversal, the actual kernel density estimates $f$
must be extracted.  This can be done by traversing the query tree and
calculating $f(p_q) = f_p(p_q) + \sum_{\mathscr{N}_i \in T}
f_n(\mathscr{N}_i)$, where $T$ is the set of nodes in $\mathscr{T}_q$ that
have $p_q$ as a descendant.  %It is possible to implement this in a way where
Each query node needs to be visited only once to perform this calculation; it
may therefore be accomplished in $O(N)$ time.

Note that this version is far simpler than other dual-tree algorithms that have
been proposed for approximate kernel density estimation \citep[see, for
instance][]{gray2003nonparametric}; however, this version is sufficient for our
runtime analysis.  Real-world implementations, such as the one found in
\textbf{mlpack} \citep{mlpack2013}, tend to be far more complex.

\begin{algorithm}[tb]
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} query point $p_q$, reference point $p_r$, list of
  kernel point estimates $\hat{f}_p$
    \STATE {\bfseries Output:} kernel value $\mathcal{K}(p_q, p_r)$

    \medskip
    \STATE $f_p(p_q) \gets f_p(p_q) + \mathcal{K}(p_q, p_r)$
\label{alg:kde-bc-update}
    \RETURN $\mathcal{K}(p_q, p_r)$
  \end{algorithmic}

  \caption{Approximate kernel density estimation \texttt{BaseCase()}}
  \label{alg:kde_base_case}
\end{algorithm}

\begin{algorithm}[tb]
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} query node $\mathscr{N}_q$, reference node
$\mathscr{N}_r$, list of node kernel estimates $\hat{f}_n$
    \STATE {\bfseries Output:} a score for the node combination $(\mathscr{N}_q,
\mathscr{N}_r)$, or $\infty$ if the combination should be pruned

    \medskip

    \IF{$\mathcal{K}(d_{\min}(\mathscr{N}_q, \mathscr{N}_r)) -
\mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r)) < \epsilon$}
\label{alg:ava-kde-prune}
      \STATE $f_n(\mathscr{N}_q) \gets f_n(\mathscr{N}_q) + |
\mathscr{D}^p(\mathscr{N}_r) | \left(\mathcal{K}(d_{\min}(\mathscr{N}_q,
\mathscr{N}_r)) + \mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r))\right)
/\;2$ \label{alg:ava-kde-update}
      \RETURN $\infty$
    \ENDIF

    \RETURN $\mathcal{K}(d_{\min}(\mathscr{N}_q, \mathscr{N}_r)) -
\mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r))$
  \end{algorithmic}

  \caption{Absolute-value approximate kernel density estimation
\texttt{Score()}}
  \label{alg:kde_score}
\end{algorithm}

\begin{thm}
Assume that $\mathcal{K}(\cdot, \cdot)$ is a kernel satisfying the assumptions
of the previous subsection.  Then, given a query set $S_q$ and a reference set
$S_r$ with expansion constant $c_r$, and using the approximate kernel density
estimation \texttt{BaseCase()} and \texttt{Score()} as given in Algorithms
\ref{alg:kde_base_case} and \ref{alg:kde_score}, respectively, with the
traversal given in Algorithm \ref{alg:cover-tree-dual}, the running time of
approximate kernel density estimation for some error parameter $\epsilon$ is
bounded by
%\begin{equation}
$O(c_r^{8 + \lceil \log_2 \zeta \rceil} (N + i_t(\mathscr{T}_q) + \theta))$
%\end{equation}
with $\zeta = -\mathcal{K}'(h) \mathcal{K}^{-1}(\epsilon) \epsilon^{-1}$,
$i_t(\mathscr{T}_q)$ defined as in Definition \ref{def:imbalance}, and $\theta$
defined as in Lemma \ref{lem:extcase3}.

\label{thm:kde-bound}
\end{thm}

\begin{proof}
It is clear that \texttt{BaseCase()} and \texttt{Score()} both take $O(1)$ time,
so Theorem \ref{thm:ct-runtime} implies the total runtime of the dual-tree
algorithm is bounded by $O(c_r^4 |R^*| (N + i_t(\mathscr{T}_q) + \theta))$.
Thus, we will bound $|R^*|$ using techniques related to those used by
\citet{ram2009}.  The bounding of $|R^*|$ is split into two sections: first,
we show that when the scale $s_r^{\max}$ is small enough, $R^*$ is empty.
Second, we bound $R^*$ when $s_r^{\max}$ is larger.

The \texttt{Score()} function is such that any node in $R^*$ for a given query
node $\mathscr{N}_q$ obeys

\begin{equation}
\mathcal{K}(d_{\min}(\mathscr{N}_q, \mathscr{N}_r)) -
\mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r))
\ge \epsilon.
\end{equation}

Thus, we are interested in the maximum possible value $\mathcal{K}(a) -
\mathcal{K}(b)$ for a fixed value of $b - a > 0$.  Due to our assumptions, the
maximum value of $\mathcal{K}'(\cdot)$ is
$\mathcal{K}'(h)$; therefore, the maximum possible value of $\mathcal{K}(a) -
\mathcal{K}(b)$ is when the interval $[a, b]$ is centered on $h$.  This allows
us to say that $\mathcal{K}(a) - \mathcal{K}(b) \le \epsilon$ when $(b - a) \le
(-\epsilon / \mathcal{K}'(h))$.  Note that

\begin{eqnarray}
d_{\max}(\mathscr{N}_q, \mathscr{N}_r) - d_{\min}(\mathscr{N}_q, \mathscr{N}_r)
&\le& d(p_q, p_r) + 2^{s_r^{\max} + 1} - d(p_q, p_r) + 2^{s_r^{\max} + 1} \\
 &\le& 2^{s_r^{\max} + 2}.
\end{eqnarray}

Therefore, $R^* = \emptyset$ when
%\begin{eqnarray}
$2^{s_r^{\max} + 2} \le -\epsilon / \mathcal{K}'(h)$, or when
%\begin{equation}
$s_r^{\max} \le \log_2( -\epsilon / \mathcal{K}'(h) ) - 2$.
%\end{equation}
%
Consider, then, the case when $s_r^{\max} > \log_2( -\epsilon /
\mathcal{K}'(h) ) - 2$.  Because of the pruning rule, for any $\mathscr{N}_r \in
R^*$, $\mathcal{K}(d_{\min}(\mathscr{N}_q, \mathscr{N}_r)) > \epsilon$;
we may refactor this by applying definitions to show
%\begin{equation}
$d(p_q, p_r) < \mathcal{K}^{-1}(\epsilon) + 2^{s_r^{\max} + 1}$.
%\end{equation}
Therefore, bounding the number of points in the set
%\begin{equation}
$B_{S_r}(p_q, \mathcal{K}^{-1}(\epsilon) + 2^{s_r^{\max} + 1}) \cap
C_{s_r^{\max}}$
%\end{equation}
is sufficient to bound $|R^*|$.  For notational convenience, define $\omega =
(\mathcal{K}^{-1}(\epsilon) / 2^{s_r^{\max} + 1}) + 1$, and the statement may be
more concisely written as $B_{S_r}(p_q, \omega 2^{s_r^{\max} + 1}) \cap
C_{s_r^{\max}}$.

Using Lemma \ref{lem:packing} with $\delta = 2^{s_r^{\max}}$ and $\rho = 2
\omega$ gives $|R^*| = c_r^{3 + \lceil \log_2 \omega \rceil}$.

%The procedure to bound this will be the same as
%for the nearest neighbor search proof---a packing bound.  See that

%\begin{eqnarray}
%B_{S_r}\left(p_q, \mathcal{K}^{-1}(\epsilon) + 2^{s_r^{\max} + 1}\right) &\subseteq&
%B_{S_r}\left(p_r, 2 (\mathcal{K}^{-1}(\epsilon) + 2^{s_r^{\max} + 1})\right) \\
% &=& B_{S_r}\left(p_r, 2^{s_r^{\max} + 2} \left(
%\frac{\mathcal{K}^{-1}(\epsilon)}{2^{s_r^{\max} + 1}} + 1\right)\right).
%\label{eqn:kde_ball}
%\end{eqnarray}

%For notational convenience, define $\omega = \mathcal{K}^{-1}(\epsilon) /
%2^{s_r^{\max} + 1} + 1$.  Applying the definition of expansion constant,

%\begin{equation}
%\left| B_{S_r}\left(p_r, 2^{s_r^{\max} + 2} \omega \right) \right|
%\le c_r^{3 + \log_2 \omega} \left|
%B_{S_r}\left(p_r, 2^{s_r^{\max} - 1}\right) \right|.
%\end{equation}

%The total number of nodes in $R^*$ is bounded by the number of disjoint balls of
%size $2^{s_r^{\max} - 1}$ that can be packed into the ball in Equation
%\ref{eqn:kde_ball}.  In the worst case, this packing is perfect, and

%\begin{equation}
%\left| B_{S_r}(p_q, \mathcal{K}^{-1}(\epsilon) + 2^{s_r^{\max} + 1}) \cap
%C_{s_r^{\max}} \right| \le \frac{\left| B_{S_r}\left(p_r, 2^{s_r^{\max} + 2}
%\omega \right) \right|}{\left| B_{S_r}\left(p_r, 2^{s_r^{\max} - 1}\right) \right|} \le c_r^{3
%+ \log_2 \omega}.
%\end{equation}

The value $\omega$ is maximized when $s_r^{\max}$ is minimized.  Using the lower
bound on $s_r^{\max}$, $\omega$ is bounded as
%
%\begin{equation}
%$\omega = \frac{\mathcal{K}^{-1}(\epsilon)}{2^{s_r^{\max} + 1} + 1} \le
%\frac{\mathcal{K}^{-1}(\epsilon)}{2^{s_r^{\max} + 1}} = -2 \mathcal{K}'(h)
%\mathcal{K}^{-1}(\epsilon) \epsilon^{-1}.
%\end{equation}
$\omega = -2 \mathcal{K}'(h) \mathcal{K}^{-1}(\epsilon) \epsilon^{-1}$.
%
Finally, with $\zeta = -\mathcal{K}'(h) \mathcal{K}^{-1}(\epsilon)
\epsilon^{-1}$, we are able to conclude that $|R^*| \le c_r^{3 + \lceil \log_2
(2 \zeta ) \rceil} = c_r^{4 + \lceil \log_2 \zeta \rceil}$.  Therefore, the
entire dual-tree traversal takes $O(c_r^{8 + \lceil \log_2 \zeta \rceil} (N +
\theta))$ time.

The postprocessing step to extract the estimates $f(\cdot)$ requires one
traversal of the tree $\mathscr{T}_r$; the tree has $O(N)$ nodes, so this takes
only $O(N)$ time.
%However, there is one more component to consider: the extraction of the
%estimates $f(p_q)$ for each $p_q$.  As discussed earlier, this process can be
%completed in one breadth-first or depth-first traversal of the query tree
%$\mathscr{T}_q$; this takes $O(N)$ time because there are $O(N)$ nodes.%: at
%each node $\mathscr{N}_q$, distribute the partial kernel density estimate to
%each of its children; for each $\mathscr{N}_c \in \mathscr{C}(\mathscr{N}_q)$,
%set
%\begin{equation}
%$f_n(\mathscr{N}_c) \gets f_n(\mathscr{N}_c) +
%f_n(\mathscr{N}_q).$
%\end{equation}
%Continue this process until the leaf nodes are reached; then, for each leaf node
%$\mathscr{N}_l$ which holds point $p_l$, take
%\begin{equation}
%$f(p_l) \gets f_p(p_l) + f_n(\mathscr{N}_l)$
%\end{equation}
%and this will produce the correct estimates $f(\cdot)$.  Because
%there are $O(N)$ nodes and each node has up to $c^4$ children, this process can
%take no longer than $O(c^4 N)$ time.
This is less than the runtime of the
dual-tree traversal, so the runtime of the dual-tree traversal dominates the
algorithm's runtime, and the theorem holds.
\end{proof}


%Define the inverse of the kernel function $\mathcal{K}(\cdot)$ as
%$\mathcal{K}^{-1}(\cdot)$, and define the set $P^*$ as the points held in each
%node in $R^*$. The set $P^*$ for a query node $\mathscr{N}_q$ with
%point $p_q$ can be split into three disjoint sets:

%\begin{eqnarray}
%P^*_l &=& \{ p_r \in P^* : d(p_q, p_r) < h - 2^{s_r^{\max} + 1} \}, \\
%P^*_m &=& \{ p_r \in P^* : h - 2^{s_r^{\max} + 1} \le d(p_q, p_r) \le h +
%2^{s_r^{\max} + 1} \}, \\
%P^*_r &=& \{ p_r \in P^* : d(p_q, p_r) > h + 2^{s_r^{\max} + 1} \}.
%\end{eqnarray}

%First, consider the set $P^*_l$.  For any $p_r \in P^*_l$, we can use the
%concavity of $\mathcal{K}(\cdot)$ in the region $[0, h]$ to see that

%\begin{eqnarray}
%\epsilon &<& \mathcal{K}(\max(0, (d(p_q, p_r) - 2^{s_r^{\max} + 1}))) -
%\mathcal{K}(d(p_q, p_r) + 2^{s_r^{\max} + 1}) \\
% &\le& \mathcal{K}(d(p_q, p_r) + 2^{s_r^{\max} + 1}) - 2^{s_r^{\max} + 2}
%\mathcal{K}'(d(p_q, p_r) + 2^{s_r^{\max} + 1}) - \mathcal{K}(d(p_q, p_r) +
%2^{s_r^{\max} + 1}) \\
% &=& -2^{s_r^{\max} + 2} \mathcal{K}'(d(p_q, p_r) + 2^{s_r^{\max} + 1}).
%\end{eqnarray}

%Denoting the inverse function of $\mathcal{K}'(\cdot)$ as
%$\mathcal{K}'^{-1}_{[a, b]}(\cdot)$ where the output value is restricted to be
%in the range $[a, b]$, we can place a lower bound on $d(p_q, p_r)$:

%\begin{equation}
%d(p_q, p_r) > \mathcal{K}'^{-1}_{[0, h]}\left( \frac{-\epsilon}{2^{s_r^{\max} +
%2}} \right) - 2^{s_r^{\max} + 1}.
%\end{equation}

%Now, consider the set $P^*_u$.  We can make a similar argument using the
%convexity of $\mathcal{K}(\cdot)$ in the region $[h, \infty]$ to see that

%\begin{eqnarray}
%\epsilon &<& \mathcal{K}(d(p_r, p_q) - 2^{s_r^{\max} + 1}) -
%\mathcal{K}(d(p_q, p_r) + 2^{s_r^{\max} + 1}) \\
% &\le& \mathcal{K}(d(p_q, p_r) - 2^{s_r^{\max} + 1}) - \left(\mathcal{K}(d(p_q,
%p_r) - 2^{s_r^{\max} + 1}) + 2^{s_r^{\max} + 2} \mathcal{K}'(d(p_q, p_r) -
%2^{s_r^{\max} + 1}) \\
% &=& -2^{s_r^{\max} + 2} \mathcal{K}'(d(p_q, p_r) - 2^{s_r^{\max} + 1}).
%\end{eqnarray}

%This leads to an upper bound on $d(p_q, p_r)$:

%\begin{equation}
%d(p_q, p_r) < \mathcal{K}'^{-1}_{[h, \infty]}\left(
%\frac{-\epsilon}{2^{s_r^{\max} + 2}}\right) + 2^{s_r^{\max} + 1}.
%\end{equation}

%Lastly, we consider the set $P^*_m$.  Here, we have that

%\begin{eqnarray}
%\epsilon &<& \mathcal{K}(d(p_q, p_r) - 2^{s_r^{\max} + 1}) - \mathcal{K}(d(p_q,
%p_r) + 2^{s_r^{\max} + 1}) \\
% &\le& -2^{s_r^{\max} + 1} \mathcal{K}'(h).
%\end{eqnarray}

%To complete the analysis, we must split into two cases for $s_r^{\max}$.  When
%$s_r^{\max} < \lfloor \log_2 \left

%In the monochromatic case, $\nu = 1$ and the bound is easier to
%understand.
The dependence on $\epsilon$ (through $\zeta$) is expected: as $\epsilon \to 0$
and the search becomes exact, $\zeta$ diverges both because $\epsilon^{-1}$
diverges and also because $\mathcal{K}^{-1}(\epsilon)$ diverges, and the runtime
goes to the worst-case $O(N^2)$; exact kernel density estimation means no nodes
can be pruned at all.

%As we will show, many kernels have a property such that $-\mathcal{K}'(h)
%\mathcal{K}^{-1}(\epsilon)$ has zero dependence on the bandwidth of the kernel.
%Although this result may seem counterintuitive, the only quantities of the
%kernel that affect the runtime are the maximum derivative (which is at the
%inflection point) and the radius of the ball for which $\mathcal{K}(\cdot) \ge
%\epsilon$.

%To see an example of the lack of dependence on kernel bandwidth, consider the
%common Gaussian kernel:

For the Gaussian kernel with bandwidth $\sigma$ defined by $\mathcal{K}_g(d) =
\exp(-d^2 / (2 \sigma^2))$, $\zeta$ does not
depend on the kernel bandwidth; only
the approximation parameter $\epsilon$.  For this kernel, $h = \sigma$ and
therefore $-\mathcal{K}'_g(h) = \sigma^{-1} e^{-1 / 2}$.  Additionally,
$\mathcal{K}_g^{-1}(\epsilon) = \sigma \sqrt{2 \ln (1 / \epsilon)}$.  This means
that for the Gaussian kernel, $\zeta = \sqrt{(-2 \ln \epsilon) / (e
\epsilon^2)}$.  Again, as $\epsilon \to 0$, the
runtime diverges; however, note that there is no dependence on the kernel
bandwidth $\sigma$.  To demonstrate the relationship of runtime to $\epsilon$,
see that for a reasonably chosen $\epsilon = 0.05$, the runtime is approximately
$O(c_r^{8.89} (N + \theta))$; for $\epsilon = 0.01$, the runtime is
approximately $O(c_r^{11.52} (N + \theta))$.  For very small $\epsilon =
0.00001$, the runtime is approximately $O(c_r^{22.15} (N + \theta))$.

Next, consider the exponential kernel:
$\mathcal{K}_l(d) = \exp(-d / \sigma)$.  For this kernel, $h = 0$ (that is, the
kernel is always convex), so then $\mathcal{K}'_l(h) = \sigma^{-1}$.  Simple
algebraic manipulation gives $\mathcal{K}^{-1}_l(\epsilon) = -\sigma \ln
\epsilon$, resulting in $\zeta = -\mathcal{K}'_l(h) \mathcal{K}^{-1}_l(\epsilon)
\epsilon^{-1} = \epsilon^{-1} \ln \epsilon$.  So both the exponential and Gaussian
kernels do not exhibit dependence on the bandwidth.

To understand the lack of dependence on kernel bandwidth more intuitively,
consider that as the kernel bandwidth increases, two things happen: {\it (a)}
the reference set $R$ becomes empty at larger scales, and {\it (b)}
$\mathcal{K}^{-1}(\epsilon)$ grows, allowing less pruning at higher levels.
These effects are opposite, and for the Gaussian and exponential kernels they
cancel each other out, giving the same bound regardless of bandwidth.

%Next, consider this sigmoidal kernel: $\mathcal{K}_s(d) = (1 + \exp(\alpha (d -
%\beta)))^{-1}$; note that this is not the more common `sigmoid kernel', which is
%actually a hyperbolic tangent.  Instead, this kernel resembles a step function.
%Some elementary calculus gives an inflection point $h = \beta$, $\mathcal{K}'(h)
%= -\alpha / 4$, and $\mathcal{K}^{-1}(\epsilon) = \ln(\epsilon^{-1} - 1) /
%\alpha + \beta$.  This gives the following expression for $\zeta$:

%\begin{equation}
%\zeta = (1/4) ( \epsilon^{-1} \ln(\epsilon^{-1} - 1) +
%(\beta \alpha) / \epsilon ).
%\end{equation}
%
%Now, consider the problem of range count, where the number of points in the
%range $[0, u]$ for each query point $p_q \in S_q$ is desired.  If the sigmoidal
%kernel is used with $\beta = u$ for approximate kernel density estimation, then
%as $\alpha \to 0$ and $\epsilon \to 0$, the results will converge to the result
%for exact range count.  Therefore, when $\alpha > 0$ or $\epsilon > 0$, the
%result is approximate range count, and we can use the results of Theorem
%\ref{thm:kde-bound} to show a runtime bound.
%
%Suppose, for the sake of simplicity, that $\alpha = O(\epsilon^{-1})$; then,
%$\zeta = O(\epsilon^{-1} \log \epsilon^{-1} + \epsilon^{-2} u) = O(\epsilon^{-2}
%u)$.  Then, the runtime of approximate range count is bounded as
%
%\begin{eqnarray}
%\begin{equation}
%O(c_r^{8 + \log_2(\epsilon^{-2} u)} \nu N) = O(c_r^{8 + 2 \log_2(\epsilon^{-1}) +
%\log_2(u)} \nu N)
% = O(\epsilon^{-1} u c_r^{8 + 3 \log_2 c_r} \nu N).
%\end{equation}
%\end{eqnarray}

%Unfortunately, this bounding technique gives a bad relationship to the
%approximation parameter $\epsilon$ (and rederivations with $\alpha \ne \epsilon$
%suffer from the same issue).  Another drawback of this technique is that it
%cannot account for a range $[l, h]$ with $l > 0$.  Therefore, in the next
%section, we develop an alternate methodology for bounding the running time of
%range search and range count.

\subsection{Relative Value Approximation}

Approximate kernel density estimation using relative-value approximation may be
bounded by reducing the absolute-value approximation algorithm (in linear time
or less) to relative-value approximation.  This is the same strategy as
performed by \citet{ram2009}.

First, we must establish a \texttt{Score()} function for relative value
approximation.  The difference between Equations \ref{eqn:ava} and \ref{eqn:rva}
is the division by the term $|f^*(p_q)|$.  But we can quickly bound
$|f^*(p_q)|$:

\begin{equation}
|f^*(p_q)| \ge N \mathcal{K}\left(\max_{p_r \in S_r} d(p_q, p_r)\right).
\end{equation}

\begin{algorithm}[tb]
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} query node $\mathscr{N}_q$, reference node
$\mathscr{N}_r$, list of node kernel estimates $\hat{f}_n$
    \STATE {\bfseries Output:} a score for the node combination $(\mathscr{N}_q,
\mathscr{N}_r)$, or $\infty$ if the combination should be pruned

    \medskip

    \IF{$\mathcal{K}(d_{\min}(\mathscr{N}_q, \mathscr{N}_r)) -
\mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r)) < \epsilon
\mathcal{K}^{\max}$}
\label{alg:rva-kde-prune}
      \STATE $f_n(\mathscr{N}_q) \gets f_n(\mathscr{N}_q) + |
\mathscr{D}^p(\mathscr{N}_r) | \left(\mathcal{K}(d_{\min}(\mathscr{N}_q,
\mathscr{N}_r)) + \mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r))\right)
/\;2$ \label{alg:rva-kde-update}
      \RETURN $\infty$
    \ENDIF

    \RETURN $\mathcal{K}(d_{\min}(\mathscr{N}_q, \mathscr{N}_r)) -
\mathcal{K}(d_{\max}(\mathscr{N}_q, \mathscr{N}_r))$
  \end{algorithmic}

  \caption{Relative-value approximate kernel density estimation
\texttt{Score()}}
  \label{alg:kde_rva_score}
\end{algorithm}

This is clearly true: each point in $S_r$ must contribute more than
$\mathcal{K}(\max_{p_r \in S_r} d(p_q, p_r))$ to $f^*(p_q)$.  Now, we may revise
the relative approximation condition in Equation \ref{eqn:rva}:

\begin{equation}
| f(p_q) - f^*(p_q) | \le \epsilon \mathcal{K}^{\max}
\end{equation}

\noindent where $\mathcal{K}^{\max}$ is lower bounded by $\mathcal{K}(\max_{p_r
\in S_r} d(p_q, p_r))$.  Assuming we have some estimate $\mathcal{K}^{\max}$,
this allows us to create a \texttt{Score()} algorithm, given in Algorithm
\ref{alg:kde_rva_score}.

Using this, we may prove linear runtime bounds for relative value approximate
kernel density estimation.

\begin{thm}
Assume that $\mathcal{K}(\cdot, \cdot)$ is a kernel satisfying the same
assumptions as Theorem \ref{thm:kde-bound}.  Then, given a query set $S_q$ and a
reference set $S_r$ both of size $O(N)$, it is possible to perform relative
value approximate kernel density estimation (satisfying the condition of
Equation \ref{eqn:rva}) in $O(N)$ time, assuming that the expansion constant
$c_r$ of $S_r$ is not dependent on $N$.
\end{thm}

\begin{proof}
It is easy to see that Theorem \ref{thm:kde-bound} may be adapted to the very
slightly different \texttt{Score()} rule of Algorithm \ref{alg:kde_rva_score}
while still providing an $O(N)$ bound.  With that \texttt{Score()} function, the
dual-tree algorithm will return relative-value approximate kernel density
estimates satisfying Equation \ref{eqn:rva}.

We now turn to the calculation of $\mathcal{K}^{\max}$.  Given the cover trees
$\mathscr{T}_q$ and $\mathscr{T}_r$ with root nodes $\mathscr{N}_{r}^{R}$ and
$\mathscr{N}_{r}^{R}$, respectively, we may calculate a suitable
$\mathcal{K}^{\max}$ value in constant time:

\begin{equation}
\mathcal{K}^{\max} = d_{\max}(\mathscr{N}_q^R, \mathscr{N}_r^R) = d(p_q^R,
p_r^R) + 2^{s_q^{\max} + 1} + 2^{s_r^{\max} + 1}.
\end{equation}

This proves the statement of the theorem.
\end{proof}

In this case, we have not shown tighter bounds because the algorithm we have
proposed is not useful in practice.  For an example of a better relative-value
approximate kernel density estimation dual-tree algorithm, see the work of
\citet{gray2003nonparametric}.
