In the previous section we have expressed a number of bounds of roughly the form

\begin{equation}
O(c^a (N + \theta))
\end{equation}

\noindent for some $a$.  Although both $c$ and $\theta$ are well-defined and
straightforward, there is limited practical utility to these bounds without
empirical experimentation.  One can easily say `these algorithms scale linearly
if $c$ has no dependence on $N$ and $\theta$ is not superlinear in $N$'; this is
useful.  But this does not answer the question of `on what datasets are these
assumptions valid, and how do these algorithms actually perform in practice?'

To help answer this oft-overlooked query, we perform an empirical analysis of
$c$, $i_t(\mathscr{T}_q)$, and $\theta$ on a variety of different datasets for
each algorithm we have presented.

\subsection{The expansion constant $c$}

\begin{table}

\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
{\bf Dataset & $n$ & $d$ & $c$ & $i_t$ & NNS & KDE & RS} \\
\hline
cloud & 2048 & 10 & 178.5 &
winequality & 6497 & 11 & 4133.0 &
birch3 & 100000 & 2 & 1866 &
corel & 37749 & 32 & 23952 &
covertype & 581012 & 54 &
isolet & 7797 & 617 & 7797 &
miniboone & 130064 & 50 & 130064 &
phy & 150000 & 78 & 27731 &
bio &
lcdm & 1808174 & 3 &
yp-msd & 515345 & 90 & 515243 &
mnist & 70000 & 784 & 70000 &
randu & 1000000 & 10 &
power & 2075260 & 7 &
\end{tabular}
\end{table}

\subsection{Tree imbalance $i_t(\mathscr{T})$}

\subsection{Nearest neighbor search}

\subsection{Kernel density estimation}

\subsection{Range search}
