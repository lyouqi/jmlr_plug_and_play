In the previous section we have expressed a number of bounds of roughly the form

\begin{equation}
O(c^a (N + \theta))
\end{equation}

\noindent for some $a$.  Although both $c$ and $\theta$ are well-defined and
straightforward, there is limited practical utility to these bounds without
empirical experimentation.  One can easily say `these algorithms scale linearly
if $c$ has no dependence on $N$ and $\theta$ is not superlinear in $N$'; this is
useful.  But this does not answer the question of `on what datasets are these
assumptions valid, and how do these algorithms actually perform in practice?'

To help answer this oft-overlooked query, we perform an empirical analysis of
$c$, $i_t(\mathscr{T}_q)$, and $\theta$ on a variety of different datasets for
each algorithm we have presented.

\subsection{The expansion constant $c$}

\subsection{Tree imbalance $i_t(\mathscr{T})$}

\subsection{Nearest neighbor search}

\subsection{Kernel density estimation}

\subsection{Range search}
